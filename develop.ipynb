{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writeup Template\n",
    "\n",
    "### You can use this file as a template for your writeup if you want to submit it as a markdown file, but feel free to use some other method and submit a pdf if you prefer.\n",
    "\n",
    "---\n",
    "\n",
    "**Advanced Lane Finding Project**\n",
    "\n",
    "The goals / steps of this project are the following:\n",
    "\n",
    "* Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.\n",
    "* Apply a distortion correction to raw images.\n",
    "* Use color transforms, gradients, etc., to create a thresholded binary image.\n",
    "* Apply a perspective transform to rectify binary image (\"birds-eye view\").\n",
    "* Detect lane pixels and fit to find the lane boundary.\n",
    "* Determine the curvature of the lane and vehicle position with respect to center.\n",
    "* Warp the detected lane boundaries back onto the original image.\n",
    "* Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position.\n",
    "\n",
    "[//]: # (Image References)\n",
    "\n",
    "[image1]: ./examples/undistort_output.png \"Undistorted\"\n",
    "[image2]: ./test_images/test1.jpg \"Road Transformed\"\n",
    "[image3]: ./examples/binary_combo_example.jpg \"Binary Example\"\n",
    "[image4]: ./examples/warped_straight_lines.jpg \"Warp Example\"\n",
    "[image5]: ./examples/color_fit_lines.jpg \"Fit Visual\"\n",
    "[image6]: ./examples/example_output.jpg \"Output\"\n",
    "[video1]: ./project_video.mp4 \"Video\"\n",
    "\n",
    "[illustration000]: ./writeup_images/0000_distorted.jpg                                 \n",
    "[illustration001]: ./writeup_images/0001_undistorted.jpg                               \n",
    "[illustration002]: ./writeup_images/0002_undistorted_plus_distorted.jpg                \n",
    "[illustration003]: ./writeup_images/0003_distorted.jpg                                 \n",
    "[illustration004]: ./writeup_images/0004_undistorted.jpg                               \n",
    "[illustration005]: ./writeup_images/0005_undistorted_plus_distorted.jpg                \n",
    "[illustration006]: ./writeup_images/0006_transform_roi.jpg                             \n",
    "[illustration007]: ./writeup_images/0007_birds_eye.jpg                                 \n",
    "[illustration008]: ./writeup_images/0008_transform_roi.jpg                             \n",
    "[illustration009]: ./writeup_images/0009_birds_eye.jpg                                 \n",
    "[illustration010]: ./writeup_images/0010_transform_roi.jpg                             \n",
    "[illustration011]: ./writeup_images/0011_birds_eye.jpg                                 \n",
    "[illustration012]: ./writeup_images/0012_hls_color_space.jpg                           \n",
    "[illustration013]: ./writeup_images/0013_luminosity.jpg                                \n",
    "[illustration014]: ./writeup_images/0014_saturation.jpg                                \n",
    "[illustration015]: ./writeup_images/0015_yellow.jpg                                    \n",
    "[illustration016]: ./writeup_images/0016_yellow_balanced.jpg                           \n",
    "[illustration017]: ./writeup_images/0017_sat_plus_yellow.jpg                           \n",
    "[illustration018]: ./writeup_images/0018_luminosity_balanced.jpg                       \n",
    "[illustration019]: ./writeup_images/0019_luminosity_highcontrast.jpg                   \n",
    "[illustration020]: ./writeup_images/0020_yellsat_balanced.jpg                          \n",
    "[illustration021]: ./writeup_images/0021_yellsat_highcontrast.jpg                      \n",
    "[illustration022]: ./writeup_images/0022_yellsat_mix_lum.jpg                           \n",
    "[illustration023]: ./writeup_images/0023_yellsat_mix_lum_highcontrast.jpg              \n",
    "[illustration024]: ./writeup_images/0024_yellsat_mix_lum_highcontrast_clamped.jpg      \n",
    "[illustration025]: ./writeup_images/0025_abs_sobelx_kernel_3.jpg                       \n",
    "[illustration026]: ./writeup_images/0026_abs_sobely_kernel_3.jpg                       \n",
    "[illustration027]: ./writeup_images/0027_gradient_angles.jpg                           \n",
    "[illustration028]: ./writeup_images/0028_gradient_magnitude.jpg                        \n",
    "[illustration029]: ./writeup_images/0029_angle_magnitude_combined.jpg                  \n",
    "[illustration030]: ./writeup_images/0030_detected_lane_pixels.jpg                      \n",
    "[illustration031]: ./writeup_images/0031_demo_lanes_detected.jpg                       \n",
    "[illustration032]: ./writeup_images/0032_pipeline_result.jpg\n",
    "\n",
    "\n",
    "\n",
    "## [Rubric](https://review.udacity.com/#!/rubrics/571/view) Points\n",
    "\n",
    "### Here I will consider the rubric points individually and describe how I addressed each point in my implementation.  \n",
    "\n",
    "---\n",
    "\n",
    "### Writeup / README\n",
    "\n",
    "#### 1. Provide a Writeup / README that includes all the rubric points and how you addressed each one.  You can submit your writeup as markdown or pdf.  [Here](https://github.com/udacity/CarND-Advanced-Lane-Lines/blob/master/writeup_template.md) is a template writeup for this project you can use as a guide and a starting point.  \n",
    "\n",
    "You're reading it!\n",
    "\n",
    "### Camera Calibration\n",
    "\n",
    "#### 1. Briefly state how you computed the camera matrix and distortion coefficients. Provide an example of a distortion corrected calibration image.\n",
    "\n",
    "The code for this step is encapsulated in `./camera_calibrator.py` two classes definition. The CameraCalibrator class should be parametrized with a calibration images folder on construction. Then, the method `calibrate()` iterates through the given folder and calls `crunch_image()` for each \".jpg\" image found. All calibration images are expected to be of the same pixel size. After a successful calibration, the calibration data (camera matrix, distortion coefficients) can be retrieved via `get_calibration_data()`. As there is only one type of usage of the calibration information, I do not store other calibration data, such as rotation and  translation vectors.\n",
    "\n",
    "The calibration process is implemented according to [this tutorial](https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_calib3d/py_calibration/py_calibration.html).\n",
    "I start by preparing \"object points\", which will be the (x, y, z) coordinates of the chessboard corners in the world. Here I am assuming the chessboard is fixed on the (x, y) plane at z=0, such that the object points are the same for each calibration image.  Thus, `objp_template` is just a replicated array of coordinates, and `objpoints` will be appended with a copy of it every time I successfully detect all chessboard corners in a test image.  `imgpoints` will be appended with the (x, y) pixel position of each of the corners in the image plane with each successful chessboard detection.  \n",
    "\n",
    "I then used the output `objpoints` and `imgpoints` to compute the camera calibration and distortion coefficients using the `cv2.calibrateCamera()` function, and store the calibration data in `mtx` and `dist` class fields respectively.\n",
    "\n",
    "To undistort an image using the calibration data, I have created a separate class `Undistorter`.\n",
    "The class should be parametrized with a calibration data and an expected image shape on construction.\n",
    "Then, any image can be undistorted, using the `undistort()` method.\n",
    "\n",
    "Original image               |  Undistorted image\n",
    ":---------------------------:|:-------------------------:\n",
    "![distorted][illustration000]|![undistorted][illustration001]\n",
    "\n",
    "Below is a mix of an original and undistorted images, where the original one is in red, and the undistorted\n",
    "in the blue channel of an image:\n",
    "\n",
    "![undistorted and distorted together][illustration002]\n",
    "\n",
    "\n",
    "### Pipeline (single images)\n",
    "\n",
    "#### 1. Provide an example of a distortion-corrected image.\n",
    "\n",
    "I have saved a grayscale distorted and undistorted road images, because that allows mixing them later using separate color channels to display the result. On a real road image, the difference between distorted and undistorted ones is not easily visible without channel mixing:\n",
    "\n",
    "Grayscale img from camera    | Undistorted grayscale road image\n",
    ":---------------------------:|:-------------------------:\n",
    "![distorted][illustration003]|![undistorted][illustration004]\n",
    "\n",
    "The resulting mix (original on red, undistorted on blue channel:\n",
    "\n",
    "![undistorted and distorted together][illustration005]\n",
    "\n",
    "As it can be easlily seen on the mixed road image, the camera distortions almost are not affecting the region where the lane lines are situated, so for this particular camera even a \"raw\", distorted image, would be good enough to find lanes and compute their curvature. However, as for a general approach, the undistortion is necessary by default.\n",
    "\n",
    "#### 2. Describe how (and identify where in your code) you performed a perspective transform and provide an example of a transformed image.\n",
    "\n",
    "# TODO: DESCRIBE CODE!\n",
    "\n",
    "I have performed tons of experiments tryng to find the best approach to reliably detect lane pixels, and then I tried to do the perspective transform **first**, before the contrast improvement and other thresholdings. This actually helped me a lot to deal with the \"challenging\" video.\n",
    "\n",
    "To perform the perspective transforms (to the \"bird-eye\" view and back from it to the \"road\" view) I have  implemented a separate class `Bird` (that has its eye, kek) in `bird.py`.\n",
    "\n",
    "While working on videos, it turned out that the \"regular\" and \"challenge\" videos have a bit different ROIs - probably, because of different camera position. So I have declared a `REGULAR_CAMERA_ROI` and a `CHALLENGE_CAMERA_ROI` arrays to use them separately.\n",
    "\n",
    "```python\n",
    "h_center = 640\n",
    "h_offset = 50\n",
    "top = 450\n",
    "bottom = 690\n",
    "left = 210\n",
    "right = 1070\n",
    "REGULAR_CAMERA_ROI = np.array([[left, bottom],\n",
    "                               [h_center - h_offset, top],\n",
    "                               [h_center + h_offset, top],\n",
    "                               [right, bottom]]\n",
    "                               , np.int32)\n",
    "```\n",
    "Please note the ROI points must be of a `np.int32` type to plot them on an image, but for the perspective  transform they must be of a `np.float32` type.\n",
    "\n",
    "An instance of Bird should be parametrized with the two regions of interest: the first one represents 4 points on a 'bird-eye' view, and the second one represents a corresponding 4 points on a camera image.\n",
    "\n",
    "For a regular video transformation, the ROIs are (starting from the bottom left point):\n",
    "\n",
    "| Source        | Destination   | \n",
    "|:-------------:|:-------------:| \n",
    "| 210, 690      | 260,  0       | \n",
    "| 590, 450      | 260,  720     |\n",
    "| 690, 450      | 1020, 720     |\n",
    "| 1070, 690     | 1020, 0       |\n",
    "\n",
    "\n",
    "I verified that my perspective transform was working as expected by drawing the `src` and `dst` points onto a test image and its warped counterpart to verify that the lines appear parallel in the warped image:\n",
    "\n",
    "Camera image (\"road\")        | Warped image (\"from_above\")\n",
    ":---------------------------:|:-------------------------:\n",
    "![road_view][illustration006]|![from_above][illustration007]\n",
    "![road_view][illustration008]|![from_above][illustration009]\n",
    "![road_view][illustration010]|![from_above][illustration011]\n",
    "\n",
    "#### 3. Describe how (and identify where in your code) you used color transforms, gradients or other methods to create a thresholded binary image.  Provide an example of a binary image result.\n",
    "\n",
    "# TODO: DESCRIBE CODE!\n",
    "\n",
    "I have tried many variations of thresholding an RGB or HLS image, but without additional efforts it is almost impossible to tell the lane line apart from the background on the \"challenging\" frames with low contrast or pavement color change.\n",
    "\n",
    "So I have developed a preprocessing pipeline, that uses:\n",
    "* \"Yellow\" channel (50% of Red + 50% of Green)\n",
    "* Saturation and luminosity channels\n",
    "* Contrast improvement\n",
    "* Vertical and close to vertical lines' pixels amplification and thresholding\n",
    "\n",
    "Also, most of the processing is being done on \"downsampled\" images. I have adjusted the downsampling (e.g. shrinking) rate to a values (about 4..8 depending on the stage) that increases processing speed without significant accuracy lost.\n",
    "\n",
    "Road view                    | Warped image (\"from_above\")\n",
    ":---------------------------:|:-------------------------:\n",
    "![road_view][illustration008]|![from_above][illustration009]\n",
    "\n",
    "\n",
    "Image in HLS (displayed in BGR) | \"Yellow\" channel (50% red + 50% green) (downsampled)\n",
    ":------------------------------:|:-------------------------:\n",
    "![road_view][illustration012]   |![from_above][illustration015]\n",
    "\n",
    "Saturation channel (ch 2 from HLS) | Luminosity channel (ch 1 from HLS)\n",
    ":------------------------------:|:-------------------------:\n",
    "![road_view][illustration014]   |![from_above][illustration013]\n",
    "\n",
    "On some road images the contrast in saturation and yellow channels is poor, and the lane marking is not bright enough (as opposed to the pavement); but anyway while we have a pavement, the lane marking would always be brighter, and, probably it would be among the brightest objects in the image.\n",
    "So I use this approach to improve contrast:\n",
    "\n",
    "1. Find the most common pixel brightness (major tone) - presuming the lane marking is brighter\n",
    "```python\n",
    "    # take histogram\n",
    "    hist = np.histogram(img)\n",
    "    # find the histogram peak == the most common brightness\n",
    "    major_tone = hist[1][np.argmax(hist[0])]\n",
    "```\n",
    "Note: the histogram is actually being taken from each N'th pixel, where N > 1, to increase the processing speed.\n",
    "\n",
    "2. Subtract this value from all pixels - this way the most part of pavement becomes black\n",
    "The result of major tone subtraction (I call it \"balanced\") is below:\n",
    "\n",
    "Yellow              | Yellow balanced \n",
    ":------------------:|:------------------:\n",
    "![][illustration015]|![][illustration016]\n",
    "\n",
    "3. After balancing the image, the lane markings (highly probable) are the brightest parts of the image, so if we square all brightnesses, the distance between dim and bright part would significantly increase. Then the result of squaring is thresholded and normalized back to range 0..255.\n",
    "\n",
    "```python\n",
    "# Assume 'luminosity' is a 0..255 np.uint8 array, containing the luminosity channel\n",
    "luminosity_highcontrast = np.minimum(np.power(luminosity.astype(np.uint16), 2), 2500))\n",
    "\n",
    "```\n",
    "After normalization back to 0..255 range, the brighter parts become much brighter, while the darker ones become black. I call this result a \"highcontrast\", here are all the \"luminosity\" channel processing stages for comparison:\n",
    "\n",
    "Luminosity as-is    | Luminosity balanced| Luminosity highcontrast\n",
    ":------------------:|:------------------:|:----------------------:\n",
    "![][illustration013]|![][illustration018]|![][illustration019]\n",
    "\n",
    "Meanwhile, the balanced yellow and the 'raw' saturation channel are summed. I have chosen this approach because the yellow lane marking might have a good brightness in both yellow channel and saturation channel, or in only one of them; but it is always much dimmer in luminosity channel (and of course is almost invisible in the blue channel).\n",
    "\n",
    "Balanced yellow     | Saturation as-is   | 50% yellow + 50% saturation\n",
    ":------------------:|:------------------:|:----------------------:\n",
    "![][illustration016]|![][illustration014]|![][illustration017]\n",
    "\n",
    "The Yellow+Saturation sum then is being made high-contrast, as described above:\n",
    "\n",
    " 50% yellow + 50% saturation | yell + sat balanced | yell + sat highcontrast\n",
    ":---------------------------:|:-------------------:|:----------------------:\n",
    "![][illustration017]         |![][illustration020] |![][illustration021]  \n",
    "\n",
    "After that we have the best possible contrast for a yellow lane marking in the `yell_sat_highcontrast`, and the best possible contrast for a white lane marking in `luminosity_highcontrast`, so I take the max value from both images to make an image where both lane markings are very bright, and the rest is dark:\n",
    "\n",
    " yell + sat highcontrast| Luminosity highcontrast | Maximum of both\n",
    ":----------------------:|:-----------------------:|:-----------------:\n",
    "![][illustration021]    |![][illustration019]     |![][illustration022]\n",
    "\n",
    "This image is again processed to highcontrast to increase signa/noise ratio for furter processing:\n",
    "\n",
    "![Highcontrast lane markings][illustration024]\n",
    "\n",
    "Now it is time to use the geometrical information to separathe just a bright object from a lane marking. Assuming we have already warped the image, the lane markings would be always close to vertical, with a small angle difference from 90 degrees. Also, the lane markings (especially after several rounds of highcontrasting) would have a very sharp edges, e.g. the brightness gradient on them would have a high absolute magnitude.\n",
    "\n",
    "To compute and threshold the angles, first take Sobel 'x' and 'y' component filters:\n",
    "\n",
    " Sobel filter, `x`  | Sobel filter, `y`\n",
    ":------------------:|:------------------:\n",
    "![][illustration025]|![][illustration026]\n",
    "\n",
    "Then compute the angle and the gradient magnitude:\n",
    "\n",
    " Probable angle     | `x+y` gradient magnitude\n",
    ":------------------:|:------------------:\n",
    "![][illustration027]|![][illustration028]\n",
    "\n",
    "Then we take only a pixels, that are both in some angle range *and* have a gradient magnitude above a threshold. That is the final result of the preprocessing pipeline:\n",
    "\n",
    "![][illustration029]\n",
    "\n",
    "\n",
    "# TODO: BELOW! ____________________\n",
    "\n",
    "\n",
    "#### 4. Describe how (and identify where in your code) you identified lane-line pixels and fit their positions with a polynomial?\n",
    "\n",
    "Then I did some other stuff and fit my lane lines with a 2nd order polynomial kinda like this:\n",
    "\n",
    "![alt text][image5]\n",
    "\n",
    "#### 5. Describe how (and identify where in your code) you calculated the radius of curvature of the lane and the position of the vehicle with respect to center.\n",
    "\n",
    "I did this in lines # through # in my code in `my_other_file.py`\n",
    "\n",
    "#### 6. Provide an example image of your result plotted back down onto the road such that the lane area is identified clearly.\n",
    "\n",
    "I implemented this step in lines # through # in my code in `yet_another_file.py` in the function `map_lane()`.  Here is an example of my result on a test image:\n",
    "\n",
    "![alt text][image6]\n",
    "\n",
    "---\n",
    "\n",
    "### Pipeline (video)\n",
    "\n",
    "#### 1. Provide a link to your final video output.  Your pipeline should perform reasonably well on the entire project video (wobbly lines are ok but no catastrophic failures that would cause the car to drive off the road!).\n",
    "\n",
    "Here's a [link to my video result](./project_video.mp4)\n",
    "\n",
    "---\n",
    "\n",
    "### Discussion\n",
    "\n",
    "#### 1. Briefly discuss any problems / issues you faced in your implementation of this project.  Where will your pipeline likely fail?  What could you do to make it more robust?\n",
    "\n",
    "Here I'll talk about the approach I took, what techniques I used, what worked and why, where the pipeline might fail and how I might improve it if I were going to pursue this project further.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_IMG_OUTPUT_PATH = \"test_images_output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "# Import everything needed to edit/save/watch video clips\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG_SAVING_MODE = True\n",
    "class ImagePrinter():\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.reset()\n",
    "        \n",
    "    def write(self, img, name):\n",
    "        if (not DEBUG_SAVING_MODE) or self.is_blocked:\n",
    "            return\n",
    "        filename = \"{:04d}_{}.jpg\".format(self.number, name)\n",
    "        self.number += 1\n",
    "        cv2.imwrite(os.path.join(self.path, filename), img)\n",
    "\n",
    "    def reset(self):\n",
    "        self.number = 0\n",
    "        self.is_blocked = False\n",
    "        \n",
    "    def block(self):\n",
    "        self.is_blocked = True\n",
    "        \n",
    "    def unblock(self):\n",
    "        self.is_blocked = False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgprinter = ImagePrinter(TEST_IMG_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIGURE_SIZE = (12, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.isdir(TEST_IMG_OUTPUT_PATH):\n",
    "    os.mkdir(TEST_IMG_OUTPUT_PATH)\n",
    "test_images = [ 'test_images/{}'.format(filename) for filename in os.listdir(\"test_images/\") ]\n",
    "test_images.sort()\n",
    "print(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from camera_calibrator import CameraCalibrator, Undistorter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrator = CameraCalibrator(9, 6)\n",
    "\n",
    "calibrator.calibrate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtx, dist = calibrator.get_calibration_data()\n",
    "print(mtx, dist)\n",
    "undistorter = Undistorter(mtx, dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgprinter.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#    Calibrator and Undistorter validation\n",
    "#\n",
    "\n",
    "for filename in ['camera_cal/calibration2.jpg', test_images[5]]:\n",
    "    img = cv2.imread(filename)\n",
    "    distorted = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    undistorted = undistorter.undistort(distorted)\n",
    "\n",
    "    out_shape = [undistorted.shape[0], undistorted.shape[1], 3]\n",
    "    output = np.zeros(out_shape, dtype=np.uint8)\n",
    "    output[:,:,0] = distorted\n",
    "    output[:,:,1] = distorted//2 + undistorted//2\n",
    "    output[:,:,2] = undistorted\n",
    "\n",
    "    imgprinter.write(distorted, \"distorted\")\n",
    "    imgprinter.write(undistorted, \"undistorted\")\n",
    "    imgprinter.write(output, \"undistorted_plus_distorted\")\n",
    "\n",
    "    plt.figure(figsize=FIGURE_SIZE)\n",
    "    plt.imshow(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bgr(img):\n",
    "    \"\"\" A helper for plotting a BGR image with matplotlib \"\"\"\n",
    "    plt.figure(figsize=FIGURE_SIZE)\n",
    "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "def plot_gray(gray):\n",
    "    plt.figure(figsize=FIGURE_SIZE)\n",
    "    plt.imshow(gray, cmap='gray')\n",
    "\n",
    "def plot_roi_on(img, roi):\n",
    "    if len(img.shape) == 2:\n",
    "        output = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "    else:\n",
    "        output = img.copy()\n",
    "    roi_color = (255, 0, 255)\n",
    "    thickness = 4\n",
    "    cv2.polylines(output, [roi], True, roi_color, thickness)\n",
    "    return output\n",
    "    \n",
    "def normalize(img):\n",
    "    \"\"\" Expects a grayscale image \"\"\"\n",
    "    minimum = np.min(img)\n",
    "    maximum = np.max(img)\n",
    "    normalized = (((img + minimum) / (maximum - minimum) ) * 255).astype(np.uint8)\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''h_center = 1280//2\n",
    "w_offset = 70\n",
    "top = 440#460#450\n",
    "bottom = 660\n",
    "left = 150\n",
    "right = 1130\n",
    "MASK_ROI = np.array([[left, bottom],\n",
    "                [h_center - w_offset, top],\n",
    "                [h_center + w_offset, top],\n",
    "                [right, bottom]]\n",
    "               , np.int32)\n",
    "mask = cv2.fillPoly(np.zeros((720,1280), dtype=np.uint8), [MASK_ROI], [255,255,255])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bird import Bird\n",
    "# Regular video best ROI is [210..1070] width, top 450 hoffs 40\n",
    "# this corresponds (from above) to 700 pix x width == 3.75m, 720 pix y height = 27 m\n",
    "h_center = 640\n",
    "h_offset = 50    # 50 for regular, 70 for challenge\n",
    "top = 450        # 450 for regular vid, 475 for challenge\n",
    "bottom = 690\n",
    "left = 210\n",
    "right = 1070\n",
    "REGULAR_CAMERA_ROI = np.array([[left, bottom],\n",
    "                [h_center - h_offset, top],\n",
    "                [h_center + h_offset, top],\n",
    "                [right, bottom]]\n",
    "               , np.int32)\n",
    "\n",
    "h_center = 640\n",
    "h_offset = 70    # 50 for regular, 70 for challenge\n",
    "top = 475        # 450 for regular vid, 475 for challenge\n",
    "bottom = 690\n",
    "left = 210\n",
    "right = 1070\n",
    "CHALLENGE_CAMERA_ROI = np.array([[left, bottom],\n",
    "                [h_center - h_offset, top],\n",
    "                [h_center + h_offset, top],\n",
    "                [right, bottom]]\n",
    "               , np.int32)\n",
    "\n",
    "CAMERA_ROI = REGULAR_CAMERA_ROI\n",
    "\n",
    "birds_top = 0\n",
    "birds_bottom = 720\n",
    "birds_left = left +50\n",
    "birds_right = right -50\n",
    "BIRD_ROI =  np.float32([[birds_left, birds_bottom],\n",
    "                       [birds_left, birds_top],\n",
    "                       [birds_right, birds_top],\n",
    "                       [birds_right, birds_bottom]])\n",
    "\n",
    "bird = Bird(BIRD_ROI, CAMERA_ROI.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the warp transform\n",
    "# https://docs.opencv.org/master/dc/da5/tutorial_py_drawing_functions.html\n",
    "for test_img in [3,6,7]: # 3,6 and 7 are good examples\n",
    "\n",
    "    img = cv2.resize(cv2.imread(test_images[test_img]), (1280, 720))\n",
    "    \n",
    "    fpv = undistorter.undistort(img)\n",
    "    \n",
    "    fpv_roi = plot_roi_on(fpv, CAMERA_ROI)\n",
    "    imgprinter.write(fpv_roi, \"transform_roi\")\n",
    "        \n",
    "    plot_bgr(fpv_roi)\n",
    "    \n",
    "    birds_eye = plot_roi_on(bird.from_above(fpv), BIRD_ROI.astype(np.int32))\n",
    "    imgprinter.write(birds_eye, \"birds_eye\")\n",
    "\n",
    "    plot_bgr(birds_eye)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtract_major_tone(img, downsample_rate=2, accuracy=10):\n",
    "    \"\"\" \n",
    "    Finds the nmost popular pixel brightness (major tone) \n",
    "    and then subtracts it from the whole image. This way we shift\n",
    "    brightness curve without adding significant artifacts (as opposed\n",
    "    to a regular thresholding)\n",
    "    \"\"\"\n",
    "    hist = np.histogram(img[::downsample_rate, ::downsample_rate], accuracy)\n",
    "    major = hist[1][np.argmax(hist[0])]\n",
    "    if (major <= 1.0):\n",
    "        # In some cases, the most popular pixel in an image is black\n",
    "        # and so there's nothing to subtract; but still we believe\n",
    "        # the lane markings to be the brightest, and just\n",
    "        # subtract the _second_ most popular pixel brightness.\n",
    "        second_max = np.partition(hist[0], -2)[-2]\n",
    "        second_max_idx = np.where(hist[0] == second_max)[0][0]\n",
    "        major = hist[1][second_max_idx]\n",
    "\n",
    "    return np.maximum(0, img.astype(np.int32) - major)\n",
    "\n",
    "def adaptive_vertical_contrast(img, downsample_rate=2, save_output=False):\n",
    "    \"\"\" Expects a cv2 mat (in BGR) as input \"\"\"\n",
    "    # The idea is to mix Lum and Sat channels to improve lane lines visibility\n",
    "    height, width = img.shape[:2]\n",
    "    hls = cv2.cvtColor(img, cv2.COLOR_BGR2HLS)\n",
    "    \n",
    "    imgprinter.write(hls, \"hls_color_space\")\n",
    "    \n",
    "    # Downsampling significantly increases processing speed\n",
    "    # while introducing only a minor artifacts\n",
    "    G = img[::downsample_rate, ::downsample_rate, 1]\n",
    "    R = img[::downsample_rate, ::downsample_rate, 2]\n",
    "    \n",
    "    quarter_lum = hls[::downsample_rate, ::downsample_rate, 1]\n",
    "    quarter_sat = hls[::downsample_rate, ::downsample_rate, 2]\n",
    "    imgprinter.write(quarter_lum, \"luminosity\")\n",
    "    imgprinter.write(quarter_sat, \"saturation\")\n",
    "\n",
    "    \n",
    "    # Yellow lane marking is the best recognizeabe as a sum of\n",
    "    # yellow brightness and saturation channel\n",
    "    yellow = R//2 + G//2\n",
    "    imgprinter.write(yellow, \"yellow\")\n",
    "    filtered_yellow = subtract_major_tone(yellow)\n",
    "    imgprinter.write(filtered_yellow, \"yellow_balanced\")\n",
    "\n",
    "    quarter_sat = quarter_sat//2 + filtered_yellow//2\n",
    "    imgprinter.write(quarter_sat, \"sat_plus_yellow\")\n",
    "    \n",
    "    min_lum = subtract_major_tone(subtract_major_tone(quarter_lum))\n",
    "    imgprinter.write(min_lum, \"luminosity_balanced\")\n",
    "    lum_part = normalize(np.minimum(np.power(min_lum, 2), 2500)) #2500\n",
    "    imgprinter.write(lum_part, \"luminosity_highcontrast\")\n",
    "\n",
    "\n",
    "    min_sat = subtract_major_tone(subtract_major_tone(quarter_sat))\n",
    "    imgprinter.write(min_sat, \"yellsat_balanced\")\n",
    "    sat_part = normalize(np.minimum(np.power(min_sat, 2), 2000))\n",
    "    imgprinter.write(sat_part, \"yellsat_highcontrast\")\n",
    "\n",
    "\n",
    "    mix = np.uint16(np.maximum(sat_part, lum_part))\n",
    "    imgprinter.write(mix, \"yellsat_mix_lum\")\n",
    "    mix = normalize(np.power(mix, 2))\n",
    "    imgprinter.write(mix, \"yellsat_mix_lum_highcontrast\")\n",
    "    \n",
    "    clamped_mix = np.maximum(0, np.minimum(mix, 255)).astype(np.uint8)\n",
    "    imgprinter.write(clamped_mix, \"yellsat_mix_lum_highcontrast_clamped\")\n",
    "    output = cv2.resize(clamped_mix, (width, height))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the HLS pipeline\n",
    "for test_img in test_images[5:6]:\n",
    "    imgprinter.block()\n",
    "    img = cv2.resize(cv2.imread(test_img), (1280, 720))\n",
    "    temp = adaptive_vertical_contrast(\n",
    "        bird.from_above(\n",
    "            undistorter.undistort(img)\n",
    "        )\n",
    "    )\n",
    "    imgprinter.unblock()\n",
    "    plot_bgr(img)\n",
    "    plot_gray(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amplify_vert_lane_pixels(gray_input, blur_radius=5, sobel_kernel=7,\n",
    "                             angle_range=(-0.5, 0.5), magnitude_range=(20,255),\n",
    "                             downsample_rate=4, save_output=False):\n",
    "    \"\"\" Finds and amplifies pixels that are looking as lane line markings\"\"\"\n",
    "    downsample = gray_input[::downsample_rate, ::downsample_rate]\n",
    "\n",
    "    # Take the absolute value of the x and y gradients\n",
    "    abs_sobelx = np.absolute(cv2.Sobel(downsample, cv2.CV_64F, 1, 0, ksize=sobel_kernel))\n",
    "    imgprinter.write(normalize(abs_sobelx), \"abs_sobelx_kernel_{}\".format(sobel_kernel))\n",
    "    abs_sobely = np.absolute(cv2.Sobel(downsample, cv2.CV_64F, 0, 1, ksize=sobel_kernel))\n",
    "    imgprinter.write(normalize(abs_sobely), \"abs_sobely_kernel_{}\".format(sobel_kernel))\n",
    "\n",
    "    # Calculate the direction of the gradient and select only pixels\n",
    "    # near edges of the matching slope (to the 'angle_range' in radians)\n",
    "    angles = np.arctan2(abs_sobely, abs_sobelx)\n",
    "    imgprinter.write(normalize(angles), \"gradient_angles\")\n",
    "    (angle_min, angle_max) = angle_range\n",
    "\n",
    "    # Calculate magnitude of the gradient\n",
    "    mag_raw = normalize(np.sqrt(np.power(abs_sobelx, 2) + np.power(abs_sobely, 2)))\n",
    "    imgprinter.write(normalize(mag_raw), \"gradient_magnitude\")\n",
    "    min_magnitude, max_magnitude = magnitude_range;\n",
    "\n",
    "    # Combining thresholds\n",
    "    combined = np.zeros_like(downsample)\n",
    "    combined[(((mag_raw > min_magnitude) & (mag_raw <= max_magnitude))\\\n",
    "              & ((angles >= angle_min) & (angles <= angle_max)))] = 255\n",
    "    imgprinter.write(normalize(combined), \"angle_magnitude_combined\")\n",
    "    output = cv2.resize(normalize(combined), (gray_input.shape[1], gray_input.shape[0]))\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrast_processing(img):\n",
    "    imgprinter.block()\n",
    "    bird_eye = bird.from_above(undistorter.undistort(img))\n",
    "    contrast = adaptive_vertical_contrast(bird_eye)\n",
    "    st_1 = contrast\n",
    "    st_2 = normalize(amplify_vert_lane_pixels(st_1, angle_range=(-0.5, 0.5),\n",
    "                                              magnitude_range=(20,255),\n",
    "                                             sobel_kernel=3))\n",
    "    return cv2.cvtColor(st_2, cv2.COLOR_GRAY2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_video = 'harder_challenge_video.mp4'; test_video_output = 'test_video_output/contrast_harder_challenge_test_video.mp4'\n",
    "input_video = 'challenge_video.mp4'; test_video_output = 'test_video_output/contrast_challenge_test_video.mp4'\n",
    "#input_video = 'project_video.mp4'; test_video_output = 'test_video_output/contrast_test_video.mp4'\n",
    "\n",
    "# 37-43 subclip of the main one is the worst\n",
    "clip1 = VideoFileClip(input_video).subclip(10,1)#48)\n",
    "#clip1 = VideoFileClip(input_video)\n",
    "white_clip = clip1.fl_image(contrast_processing)\n",
    "%time white_clip.write_videofile(test_video_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Validate the HLS pipeline and plot pipeline frames\n",
    "for test_img in test_images[5:6]:\n",
    "    imgprinter.unblock()\n",
    "    img = cv2.resize(cv2.imread(test_img), (1280, 720))\n",
    "    bird_eye = bird.from_above(undistorter.undistort(img))\n",
    "    contrast = adaptive_vertical_contrast(bird_eye)\n",
    "    st_1 = contrast\n",
    "    st_2 = normalize(amplify_vert_lane_pixels(st_1, angle_range=(-0.5, 0.5),\n",
    "                                              magnitude_range=(20,255),\n",
    "                                             sobel_kernel=3))\n",
    "    plot_bgr(img)\n",
    "    plot_gray(st_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define conversions in x and y from pixels space to meters\n",
    "ym_per_pix = 27/720 # meters per pixel in y dimension for the regular video\n",
    "xm_per_pix = 3.75/713 # meters per pixel in x dimension for the regular video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_poly2(poly, y):\n",
    "    ''' Evaluate a 2-grade polynomial '''\n",
    "    return poly[0]*y*y + poly[1]*y + poly[2]\n",
    "\n",
    "def get_lane_template_birdeye(shape, lane_width = 60):\n",
    "    width = shape[1]\n",
    "    left_line_desired_position = width * 1 // 4\n",
    "    right_line_desired_position = width * 3 // 4\n",
    "    salted = np.zeros(shape, dtype=np.uint8)\n",
    "    salted[:, left_line_desired_position-lane_width:left_line_desired_position+lane_width] = 255\n",
    "    salted[:, right_line_desired_position-lane_width:right_line_desired_position+lane_width] = 255\n",
    "    return salted\n",
    "\n",
    "def find_lane_pixels(binary_warped):\n",
    "    # Take a histogram of the bottom half of the image\n",
    "    histogram = np.sum(binary_warped[binary_warped.shape[0]//2:,:], axis=0)\n",
    "    # Create an output image to draw on and visualize the result\n",
    "    out_img = np.dstack((binary_warped, binary_warped, binary_warped))\n",
    "    # Find the peak of the left and right halves of the histogram\n",
    "    # These will be the starting point for the left and right lines\n",
    "    midpoint = np.int(histogram.shape[0]//2)\n",
    "    leftx_base = np.argmax(histogram[:midpoint])\n",
    "    rightx_base = np.argmax(histogram[midpoint:]) + midpoint\n",
    "\n",
    "    # HYPERPARAMETERS\n",
    "    # Choose the number of sliding windows\n",
    "    nwindows = 9\n",
    "    # Set the width of the windows +/- margin\n",
    "    margin = 100\n",
    "    # Set minimum number of pixels found to recenter window\n",
    "    minpix = 50\n",
    "\n",
    "    # Set height of windows - based on nwindows above and image shape\n",
    "    window_height = np.int(binary_warped.shape[0]//nwindows)\n",
    "    # Identify the x and y positions of all nonzero pixels in the image\n",
    "    nonzero = binary_warped.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "    # Current positions to be updated later for each window in nwindows\n",
    "    leftx_current = leftx_base\n",
    "    rightx_current = rightx_base\n",
    "\n",
    "    # Create empty lists to receive left and right lane pixel indices\n",
    "    left_lane_inds = []\n",
    "    right_lane_inds = []\n",
    "\n",
    "    # Step through the windows one by one\n",
    "    for window in range(nwindows):\n",
    "        # Identify window boundaries in x and y (and right and left)\n",
    "        win_y_low = binary_warped.shape[0] - (window+1)*window_height\n",
    "        win_y_high = binary_warped.shape[0] - window*window_height\n",
    "        win_xleft_low = leftx_current - margin \n",
    "        win_xleft_high = leftx_current + margin\n",
    "        win_xright_low = rightx_current - margin\n",
    "        win_xright_high = rightx_current + margin\n",
    "        # Draw the windows on the visualization image\n",
    "        cv2.rectangle(out_img,(win_xleft_low,win_y_low),\n",
    "                      (win_xleft_high, win_y_high),(0,255,0), 2) \n",
    "        cv2.rectangle(out_img,(win_xright_low,win_y_low),\n",
    "                      (win_xright_high, win_y_high),(0,255,0), 2) \n",
    "        \n",
    "        good_left_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & \n",
    "        (nonzerox >= win_xleft_low) &  (nonzerox < win_xleft_high)).nonzero()[0]\n",
    "        good_right_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & \n",
    "        (nonzerox >= win_xright_low) &  (nonzerox < win_xright_high)).nonzero()[0]\n",
    "        \n",
    "        # Append these indices to the lists\n",
    "        left_lane_inds.append(good_left_inds)\n",
    "        right_lane_inds.append(good_right_inds)\n",
    "        \n",
    "        if len(good_left_inds) > minpix:\n",
    "            leftx_current = np.int(np.mean(nonzerox[good_left_inds]))\n",
    "        if len(good_right_inds) > minpix: \n",
    "            rightx_current = np.int(np.mean(nonzerox[good_right_inds]))\n",
    "\n",
    "    # Concatenate the arrays of indices (previously was a list of lists of pixels)\n",
    "    left_lane_inds = np.concatenate(left_lane_inds)\n",
    "    right_lane_inds = np.concatenate(right_lane_inds)\n",
    "\n",
    "    # Extract left and right line pixel positions\n",
    "    leftx = nonzerox[left_lane_inds]\n",
    "    lefty = nonzeroy[left_lane_inds] \n",
    "    rightx = nonzerox[right_lane_inds]\n",
    "    righty = nonzeroy[right_lane_inds]\n",
    "    imgprinter.write(out_img, \"lane_pixel_finding\")\n",
    "    return leftx, lefty, rightx, righty\n",
    "\n",
    "def take_lane_pixels(binary_warped, lcurve, rcurve, margin=50, downsample_rate=2):\n",
    "    \"\"\"\n",
    "    Expects lcurve and rcurve as array of points.\n",
    "    Vertical stride along the curve must be the same, and must be\n",
    "    equal across both curves.\n",
    "    These points are used as window origins: the point is a [bottom, h_center] of\n",
    "    a window.\n",
    "    Curves must be consistent with an image (e.g. of the same height and compatible width)\n",
    "    Not all of the pixels might be taken into account if the downsample rate is > 1\n",
    "    (this way further polynome fitting becomes faster)\n",
    "    \"\"\"\n",
    "    assert(len(lcurve) > 2 and len(rcurve) > 2)\n",
    "    l_v_stride = lcurve[1][1] - lcurve[0][1]\n",
    "    r_v_stride = rcurve[1][1] - rcurve[0][1]\n",
    "    assert(l_v_stride == r_v_stride)\n",
    "    v_stride = l_v_stride\n",
    "    leftx = []; lefty = []; rightx = []; righty = []\n",
    "    # Identify the x and y positions of all nonzero pixels in the image\n",
    "    nonzero = binary_warped.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "    for point in lcurve:\n",
    "        x, y = point\n",
    "        w_left = x - margin; w_right = x + margin\n",
    "        w_top = y; w_bottom = y + v_stride\n",
    "        window = binary_warped[w_top:w_bottom, w_left:w_right]\n",
    "        w_y_indices, w_x_indices = window.nonzero()\n",
    "        if len(w_x_indices > 0):\n",
    "            leftx.append(w_x_indices + w_left-1)\n",
    "            lefty.append(w_y_indices + w_top-1)\n",
    "        # DEBUG!\n",
    "        # binary_warped[y:y+v_stride:2, w_left:w_right:2] = 255\n",
    "    for point in rcurve:\n",
    "        x, y = point\n",
    "        w_left = x - margin; w_right = x + margin\n",
    "        w_top = y; w_bottom = y + v_stride\n",
    "        window = binary_warped[w_top:w_bottom, w_left:w_right]\n",
    "        w_y_indices, w_x_indices = window.nonzero()\n",
    "        if len(w_x_indices > 0):\n",
    "            rightx.append(w_x_indices + w_left-1)\n",
    "            righty.append(w_y_indices + w_top-1)\n",
    "        # DEBUG!\n",
    "        # binary_warped[y:y+v_stride:2, w_left:w_right:2] = 255\n",
    "    leftx = np.concatenate(leftx) if len(leftx) > 0 else np.uint16([])\n",
    "    lefty = np.concatenate(lefty) if len(lefty) > 0 else np.uint16([])\n",
    "    rightx = np.concatenate(rightx) if len(rightx) > 0 else np.uint16([])\n",
    "    righty = np.concatenate(righty) if len(righty) > 0 else np.uint16([])\n",
    "    return leftx, lefty, rightx, righty\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "image_tricolor = cv2.imread(\"bird_eye.png\")\n",
    "test_takepixel_img = cv2.cvtColor(image_tricolor, cv2.COLOR_BGR2GRAY)\n",
    "test_takepixel_img.shape\n",
    "\n",
    "lx,ly,rx,ry = find_lane_pixels(test_takepixel_img)\n",
    "lfit, rfit = fit_polynomial(birds_eye, lx, ly, rx, ry, xm_per_pix, ym_per_pix)\n",
    "lcurve, rcurve = get_plottable_curves(img.shape[0], lfit, rfit, xm_per_pix, ym_per_pix)\n",
    "\n",
    "lx2,ly2,rx2,ry2 = take_lane_pixels(test_takepixel_img, lcurve, rcurve)\n",
    "\n",
    "testplot = np.zeros_like(image_tricolor)\n",
    "testplot[ly2, lx2, 0] = 255\n",
    "testplot[:,:,1] = test_takepixel_img\n",
    "testplot[ry2, rx2, 2] = 255\n",
    "\n",
    "\n",
    "plot_bgr(test_takepixel_img)\n",
    "plot_bgr(testplot)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_polynomial(binary_warped, leftx, lefty, rightx, righty, xm_per_pix, ym_per_pix):\n",
    "    if (len(leftx) == 0 or len(lefty) == 0 or len(rightx) == 0 or len(righty) == 0):\n",
    "        salted = get_lane_template_birdeye(binary_warped.shape)\n",
    "        leftx, lefty, rightx, righty = find_lane_pixels(salted)\n",
    "    left_fit = np.polyfit(lefty*ym_per_pix, leftx*xm_per_pix, 2)\n",
    "    right_fit = np.polyfit(righty*ym_per_pix, rightx*xm_per_pix, 2)\n",
    "    \n",
    "    return left_fit, right_fit\n",
    "\n",
    "def get_plottable_curves(height, left_fit, right_fit, xm_per_pix, ym_per_pix, steps=20):\n",
    "    ploty = np.linspace(0, height, steps)\n",
    "    left_curve = np.int32(list(zip(\n",
    "        evaluate_poly2(left_fit, ploty * ym_per_pix) / xm_per_pix, ploty)))\n",
    "    right_curve = np.int32(list(zip(\n",
    "        evaluate_poly2(right_fit, ploty * ym_per_pix) / xm_per_pix, ploty)))\n",
    "    return left_curve, right_curve\n",
    "    \n",
    "def plot_lane_curves(lane_img, left_curve, right_curve, thickness=6):\n",
    "    out_img = cv2.polylines(lane_img, [left_curve], False, [0,255,255], thickness)\n",
    "    out_img = cv2.polylines(out_img, [right_curve], False, [255,255,0], thickness)\n",
    "    return out_img\n",
    "\n",
    "def plot_lane_poly_on(lane_img, left_curve, right_curve, color=[100,200,100]):\n",
    "    # Right curve's points must go in the opposite order to maintain a\n",
    "    # polygon's points traversing order.\n",
    "    points = np.append(left_curve, right_curve[::-1], axis=0)\n",
    "    cv2.fillPoly(lane_img, [points], color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lane_line import LaneLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_circles(img, lcurv, rcurv):\n",
    "    avg_curv = (lcurv + rcurv) / 2\n",
    "    r_color = [0, 0, 255]\n",
    "    l_color = [255, 0, 255]\n",
    "    thickness = 15\n",
    "    height, width = img.shape[:2]\n",
    "    h_center = width // 2\n",
    "    left_avg_center = (int(h_center - avg_curv), height)\n",
    "    right_avg_center = (int(h_center + avg_curv), height)\n",
    "    cv2.circle(img, left_avg_center, int(avg_curv), l_color, thickness)\n",
    "    cv2.circle(img, right_avg_center, int(avg_curv), r_color, thickness)\n",
    "    \n",
    "def plot_curvatures_on(img, lcurvature, rcurvature, color=(20, 20, 20), fontScale = 1.3, thickness = 3):\n",
    "    height, width = img.shape[:2]\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX \n",
    "    \n",
    "    # As the curvature radius calculation is not precise, I round\n",
    "    # results to 10 meters (e.g. to the second integer digit)\n",
    "    message = \"Lane radius: ~{:4.0f} m    (left {:4.0f} m, right {:4.0f} m)\".format(\n",
    "                                round((lcurvature + rcurvature) / 2, -1),\n",
    "                                round(lcurvature, -1),\n",
    "                                round(rcurvature, -1), )\n",
    "\n",
    "    origin = (int(width * 0.05), int(height * 0.1)) \n",
    "    output = cv2.putText(img, message, origin, font, fontScale, (200,200,200), 15, cv2.LINE_AA)\n",
    "    return cv2.putText(output, message, origin, font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "\n",
    "def plot_hcenter_offset_on(img, loffset, roffset, thickness = 1):\n",
    "    offset = loffset + roffset\n",
    "    height, width = img.shape[:2]\n",
    "    green = (50, 150, 50)\n",
    "    blue = (150, 50, 50)\n",
    "    message = \"Offset {:4.0f} mm\" .format(round(abs(offset), -1))\n",
    "    r_origin = (int(width * 0.52), int(height * 0.9)) \n",
    "    l_origin = (int(width * 0.20), int(height * 0.9)) \n",
    "    origin = r_origin if (offset > 0) else l_origin\n",
    "    color = green if (offset > 0) else blue\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX \n",
    "    fontScale = 1.2; thickness = 2\n",
    "    output = cv2.putText(img, message, origin, font, fontScale, (200,200,200), 10, cv2.LINE_AA)\n",
    "    return cv2.putText(output, message, origin, font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate lane pixel finding via sliding window\n",
    "for test_img in test_images[5:4]:\n",
    "    imgprinter.block()\n",
    "    img = cv2.resize(undistorter.undistort(cv2.imread(test_img)), (1280, 720))\n",
    "    hls = adaptive_vertical_contrast(bird.from_above(img))\n",
    "    highlight = amplify_vert_lane_pixels(hls)\n",
    "    birds_eye = highlight\n",
    "    lx, ly, rx, ry = find_lane_pixels(birds_eye)\n",
    "    cv2.imwrite(\"bird_eye.png\", birds_eye)\n",
    "    lfit, rfit = fit_polynomial(birds_eye, lx, ly, rx, ry, xm_per_pix, ym_per_pix)\n",
    "    lcurve, rcurve = get_plottable_curves(img.shape[0], lfit, rfit, xm_per_pix, ym_per_pix)\n",
    "    lx, ly, rx, ry = take_lane_pixels(birds_eye, lcurve, rcurve)\n",
    "    lfit2, rfit2 = fit_polynomial(birds_eye, lx, ly, rx, ry, xm_per_pix, ym_per_pix)\n",
    "    lcurve2, rcurve2 = get_plottable_curves(img.shape[0], lfit2, rfit2, xm_per_pix, ym_per_pix)\n",
    "\n",
    "    found = cv2.cvtColor(birds_eye, cv2.COLOR_GRAY2BGR)\n",
    "    found[ly, lx] = [50, 50 , 200]\n",
    "    found[ry, rx] = [50, 200, 50]\n",
    "    found = plot_lane_curves(found, lcurve, rcurve)\n",
    "    found = plot_lane_curves(found, lcurve2, rcurve2)\n",
    "\n",
    "    lcurv = LaneLine.curvature(lfit, 720 * ym_per_pix)\n",
    "    rcurv = LaneLine.curvature(rfit, 720 * ym_per_pix)\n",
    "      \n",
    "    imgprinter.unblock()\n",
    "    plot_bgr(found)\n",
    "    imgprinter.write(found, \"detected_lane_pixels\")\n",
    "    \n",
    "    lane_curves_from_above = plot_lane_curves(np.zeros_like(found), lcurve, rcurve, thickness=40)\n",
    "    plot_bgr(lane_curves_from_above)\n",
    "    \n",
    "    lane_curves_on_road = bird.to_road(lane_curves_from_above)\n",
    "    demo = np.uint8((img.astype(np.uint16) + lane_curves_on_road)//2)\n",
    "    plot_bgr(demo)\n",
    "    imgprinter.write(demo, \"demo_lanes_detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: an Undistorter instance is necessary\n",
    "# with avg 10 lanes work. L8 + Buf32(_20) is ok too.\n",
    "# Avg6 + Buf32(_20) works too.\n",
    "# Avg4 + Buf8(_40) for regular video is ok\n",
    "# Avg8 + Buf8(_20) for challenge is ok too.\n",
    "rlane = LaneLine(xm_per_pix, ym_per_pix, avg_depth=8, max_valid_diff=0.75, max_broken_frames=12, name=\"Right\")\n",
    "llane = LaneLine(xm_per_pix, ym_per_pix, avg_depth=8, max_valid_diff=0.75, max_broken_frames=12, name=\"Left\")\n",
    "\n",
    "buffer_frame = None\n",
    "buf_avg = 12\n",
    "\n",
    "def process_frame(input_frame, plot_everything=False):\n",
    "    if input_frame.shape[:2] != [720, 1280]:\n",
    "        frame = cv2.resize(input_frame, (1280, 720))\n",
    "    else:\n",
    "        frame = input_frame\n",
    "    imgprinter.block()\n",
    "    global buffer_frame, buf_avg\n",
    "    global rlane, llane\n",
    "    \n",
    "    is_all_ok = (llane.is_valid() and rlane.is_valid())\n",
    "    downsample_rate = 2\n",
    "    \n",
    "    \"\"\" Expects an RGB frame as input \"\"\"\n",
    "    height, width, _ = frame.shape\n",
    "    birds_eye = bird.from_above(undistorter.undistort(frame))\n",
    "    stage_0_sample = adaptive_vertical_contrast(birds_eye, downsample_rate=downsample_rate)\n",
    "    stage_1_sample = amplify_vert_lane_pixels(stage_0_sample, downsample_rate=downsample_rate)\n",
    "    stage_2_sample = normalize(stage_1_sample)\n",
    "    \n",
    "    if buffer_frame is None:\n",
    "        buffer_frame = np.uint32(stage_2_sample)\n",
    "    else:\n",
    "        buffer_frame = (buffer_frame * (buf_avg - 1) + stage_2_sample) / buf_avg\n",
    "\n",
    "    filtered_stage2 = np.zeros_like(stage_2_sample)\n",
    "    filtered_stage2[buffer_frame > 20] = 1\n",
    "    filtered_stage2 = cv2.morphologyEx(filtered_stage2, cv2.MORPH_OPEN, np.ones((5,5)))\n",
    "\n",
    "    if is_all_ok:\n",
    "        # Faster\n",
    "        downsample_rate = 10\n",
    "        lx, ly, rx, ry = take_lane_pixels(filtered_stage2,\n",
    "                                          llane.get_curve(),\n",
    "                                          rlane.get_curve(),\n",
    "                                          margin=80)\n",
    "    else:\n",
    "        # More robust\n",
    "        downsample_rate = 2\n",
    "        lx, ly, rx, ry = find_lane_pixels(filtered_stage2)\n",
    "    \n",
    "    # To fit a polynome curve, we need as few points as possible, as it \n",
    "    # speeds up computations; but not too few to keep the accuracy.\n",
    "    fit_downsample = 1 + max(len(lx), len(ly)) // 1000\n",
    "    lfit, rfit = fit_polynomial(filtered_stage2,\n",
    "                                lx[::fit_downsample],\n",
    "                                ly[::fit_downsample],\n",
    "                                rx[::fit_downsample],\n",
    "                                ry[::fit_downsample],\n",
    "                                xm_per_pix, ym_per_pix)\n",
    "    llane.update(lfit); \n",
    "    rlane.update(rfit);\n",
    "    \n",
    "    lcurve, rcurve = get_plottable_curves(height, llane.get_fit(), rlane.get_fit()\n",
    "                                          , xm_per_pix, ym_per_pix, steps=15)\n",
    "    llane.set_curve(lcurve)\n",
    "    rlane.set_curve(rcurve)\n",
    "    \n",
    "    \n",
    "    stage_3_sample = plot_lane_curves(cv2.cvtColor(normalize(filtered_stage2),\n",
    "                                                   cv2.COLOR_GRAY2BGR)\n",
    "                                      , lcurve, rcurve)\n",
    "    \n",
    "    road_poly_from_above = np.zeros_like(frame)\n",
    "    poly_color = [100,200,100] if is_all_ok else [255,0,0]\n",
    "    plot_lane_poly_on(road_poly_from_above, lcurve, rcurve, color=poly_color)\n",
    "    lane_poly = bird.to_road(road_poly_from_above)\n",
    "    detection_result = np.maximum(frame, lane_poly)\n",
    "    plot_curvatures_on(detection_result, llane.get_radius(), rlane.get_radius())\n",
    "    plot_hcenter_offset_on(detection_result, llane.get_horizontal_offset(), rlane.get_horizontal_offset())\n",
    "\n",
    "    if not plot_everything:\n",
    "        return detection_result\n",
    "            \n",
    "    tile_00 = cv2.resize(detection_result, (width//2, height//2))\n",
    "    tile_01 = cv2.cvtColor(cv2.resize(stage_0_sample, (width//2, height//2)), cv2.COLOR_GRAY2BGR)\n",
    "    tile_10 = cv2.cvtColor(cv2.resize(stage_1_sample, (width//2, height//2)), cv2.COLOR_GRAY2BGR)\n",
    "    tile_11 = cv2.resize(stage_3_sample, (width//2, height//2))\n",
    "\n",
    "    output = np.zeros_like(frame)\n",
    "    output [:height//2, :width//2] = tile_00\n",
    "    output [:height//2, width//2:] = tile_01\n",
    "    output [height//2:, :width//2] = tile_10\n",
    "    output [height//2:, width//2:] = tile_11\n",
    "    imgprinter.unblock()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate lane pixel finding via sliding window\n",
    "for test_img in test_images[:]:\n",
    "    for i in range(0, 10): # processing the same frame several times to trigger averaging\n",
    "        img = cv2.resize(cv2.imread(test_img), (1280, 720))\n",
    "        process_frame(img)\n",
    "    result = process_frame(img)\n",
    "    plot_bgr(result)\n",
    "    cv2.imwrite(test_img.replace('test_images','output_images'), result)\n",
    "    #imgprinter.unblock()\n",
    "    #imgprinter.write(result, \"pipeline_result\")\n",
    "    buffer_frame = None\n",
    "    llane.reset()\n",
    "    rlane.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#input_video = 'harder_challenge_video.mp4'; test_video_output = 'test_video_output/harder_challenge_test_video.mp4'\n",
    "#input_video = 'challenge_video.mp4'; test_video_output = 'test_video_output/challenge_test_video.mp4'\n",
    "#input_video = 'project_video.mp4'; test_video_output = 'test_video_output/test_video.mp4'\n",
    "input_video = 'solidWhiteRight.mp4'; test_video_output = 'test_video_output/solidWhiteRight.mp4'\n",
    "imgprinter.block()\n",
    "# 37-43 subclip of the main one is the worst, challenge problem on 3..6\n",
    "clip1 = VideoFileClip(input_video).subclip(1,5)#48)\n",
    "#clip1 = VideoFileClip(input_video)\n",
    "white_clip = clip1.fl_image(process_frame)\n",
    "%time white_clip.write_videofile(test_video_output, audio=False)\n",
    "imgprinter.unblock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
